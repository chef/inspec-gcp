# frozen_string_literal: false

# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
#     Please read more about how to change this file in README.md and
#     CONTRIBUTING.md located at the root of this package.
#
# ----------------------------------------------------------------------------
require 'gcp_backend'
class DataprocJobs < GcpResourceBase
  name 'google_dataproc_jobs'
  desc 'Job plural resource'
  supports platform: 'gcp'

  attr_reader :table

  filter_table_config = FilterTable.create

  filter_table_config.add(:references, field: :reference)
  filter_table_config.add(:placements, field: :placement)
  filter_table_config.add(:hadoop_jobs, field: :hadoop_job)
  filter_table_config.add(:spark_jobs, field: :spark_job)
  filter_table_config.add(:pyspark_jobs, field: :pyspark_job)
  filter_table_config.add(:hive_jobs, field: :hive_job)
  filter_table_config.add(:pig_jobs, field: :pig_job)
  filter_table_config.add(:spark_r_jobs, field: :spark_r_job)
  filter_table_config.add(:spark_sql_jobs, field: :spark_sql_job)
  filter_table_config.add(:presto_jobs, field: :presto_job)
  filter_table_config.add(:trino_jobs, field: :trino_job)
  filter_table_config.add(:flink_jobs, field: :flink_job)
  filter_table_config.add(:statuses, field: :status)
  filter_table_config.add(:status_histories, field: :status_history)
  filter_table_config.add(:yarn_applications, field: :yarn_applications)
  filter_table_config.add(:driver_output_resource_uris, field: :driver_output_resource_uri)
  filter_table_config.add(:driver_control_files_uris, field: :driver_control_files_uri)
  filter_table_config.add(:labels, field: :labels)
  filter_table_config.add(:schedulings, field: :scheduling)
  filter_table_config.add(:job_uuids, field: :job_uuid)
  filter_table_config.add(:dones, field: :done)
  filter_table_config.add(:driver_scheduling_configs, field: :driver_scheduling_config)

  filter_table_config.connect(self, :table)

  def initialize(params = {})
    super(params.merge({ use_http_transport: true }))
    @params = params
    @table = fetch_wrapped_resource('jobs')
  end

  def fetch_wrapped_resource(wrap_path)
    # fetch_resource returns an array of responses (to handle pagination)
    result = @connection.fetch_all(product_url, resource_base_url, @params, 'Get')
    return if result.nil?

    # Conversion of string -> object hash to symbol -> object hash that InSpec needs
    converted = []
    result.each do |response|
      next if response.nil? || !response.key?(wrap_path)
      response[wrap_path].each do |hash|
        hash_with_symbols = {}
        hash.each_key do |key|
          name, value = transform(key, hash)
          hash_with_symbols[name] = value
        end
        converted.push(hash_with_symbols)
      end
    end

    converted
  end

  def transform(key, value)
    return transformers[key].call(value) if transformers.key?(key)

    [key.to_sym, value]
  end

  def transformers
    {
      'reference' => ->(obj) { [:reference, GoogleInSpec::Dataproc::Property::JobReference.new(obj['reference'], to_s)] },
      'placement' => ->(obj) { [:placement, GoogleInSpec::Dataproc::Property::JobPlacement.new(obj['placement'], to_s)] },
      'hadoopJob' => ->(obj) { [:hadoop_job, GoogleInSpec::Dataproc::Property::JobHadoopJob.new(obj['hadoopJob'], to_s)] },
      'sparkJob' => ->(obj) { [:spark_job, GoogleInSpec::Dataproc::Property::JobSparkJob.new(obj['sparkJob'], to_s)] },
      'pysparkJob' => ->(obj) { [:pyspark_job, GoogleInSpec::Dataproc::Property::JobPysparkJob.new(obj['pysparkJob'], to_s)] },
      'hiveJob' => ->(obj) { [:hive_job, GoogleInSpec::Dataproc::Property::JobHiveJob.new(obj['hiveJob'], to_s)] },
      'pigJob' => ->(obj) { [:pig_job, GoogleInSpec::Dataproc::Property::JobPigJob.new(obj['pigJob'], to_s)] },
      'sparkRJob' => ->(obj) { [:spark_r_job, GoogleInSpec::Dataproc::Property::JobSparkRJob.new(obj['sparkRJob'], to_s)] },
      'sparkSqlJob' => ->(obj) { [:spark_sql_job, GoogleInSpec::Dataproc::Property::JobSparkSqlJob.new(obj['sparkSqlJob'], to_s)] },
      'prestoJob' => ->(obj) { [:presto_job, GoogleInSpec::Dataproc::Property::JobPrestoJob.new(obj['prestoJob'], to_s)] },
      'trinoJob' => ->(obj) { [:trino_job, GoogleInSpec::Dataproc::Property::JobTrinoJob.new(obj['trinoJob'], to_s)] },
      'flinkJob' => ->(obj) { [:flink_job, GoogleInSpec::Dataproc::Property::JobFlinkJob.new(obj['flinkJob'], to_s)] },
      'status' => ->(obj) { [:status, GoogleInSpec::Dataproc::Property::JobStatus.new(obj['status'], to_s)] },
      'statusHistory' => ->(obj) { [:status_history, GoogleInSpec::Dataproc::Property::JobStatusHistoryArray.parse(obj['statusHistory'], to_s)] },
      'yarnApplications' => ->(obj) { [:yarn_applications, GoogleInSpec::Dataproc::Property::JobYarnApplicationsArray.parse(obj['yarnApplications'], to_s)] },
      'driverOutputResourceUri' => ->(obj) { [:driver_output_resource_uri, obj['driverOutputResourceUri']] },
      'driverControlFilesUri' => ->(obj) { [:driver_control_files_uri, obj['driverControlFilesUri']] },
      'labels' => ->(obj) { [:labels, GoogleInSpec::Dataproc::Property::JobLabels.new(obj['labels'], to_s)] },
      'scheduling' => ->(obj) { [:scheduling, GoogleInSpec::Dataproc::Property::JobScheduling.new(obj['scheduling'], to_s)] },
      'jobUuid' => ->(obj) { [:job_uuid, obj['jobUuid']] },
      'done' => ->(obj) { [:done, obj['done']] },
      'driverSchedulingConfig' => ->(obj) { [:driver_scheduling_config, GoogleInSpec::Dataproc::Property::JobDriverSchedulingConfig.new(obj['driverSchedulingConfig'], to_s)] },
    }
  end

  private

  def product_url(_ = nil)
    'https://dataproc.googleapis.com/v1/'
  end

  def resource_base_url
    'projects/{{project_id}}/regions/{{region}}/jobs'
  end
end
